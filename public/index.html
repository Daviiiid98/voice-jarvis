<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>JARVIS Voice</title>
    <script src="https://telegram.org/js/telegram-web-app.js"></script>
    <style>
        body {
            margin: 0;
            padding: 0;
            background-color: #000;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            font-family: 'Courier New', Courier, monospace;
        }

        canvas {
            position: absolute;
            top: 0;
            left: 0;
            z-index: 1;
        }

        #ui-layer {
            position: relative;
            z-index: 10;
            text-align: center;
            pointer-events: none; /* Чтобы клики проходили сквозь текст */
            width: 100%;
            height: 100%;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }

        .status-text {
            color: #00f3ff;
            text-shadow: 0 0 10px #00f3ff;
            font-size: 14px;
            letter-spacing: 2px;
            margin-top: 250px; /* Отступ от центра сферы */
            text-transform: uppercase;
        }

        /* Невидимая кнопка поверх всего для клика */
        #click-zone {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 20;
            cursor: pointer;
            /* Убираем подсветку при тапе на мобильных */
            -webkit-tap-highlight-color: transparent;
        }
    </style>
</head>
<body>

    <canvas id="canvas"></canvas>

    <div id="ui-layer">
        <div class="status-text" id="status">SYSTEM OFFLINE<br>TAP TO INITIALIZE</div>
    </div>

    <div id="click-zone"></div>

    <script>
        // --- НАСТРОЙКИ ВИЗУАЛА (JARVIS STYLE) ---
        const PARTICLE_COUNT = 150;
        const CONNECT_DISTANCE = 80;
        const CORE_RADIUS = 60;
        const COLOR_IDLE = 'rgba(0, 243, 255, '; // Cyan
        const COLOR_SPEAKING = 'rgba(255, 0, 100, '; // Red/Pink alert
        const COLOR_LISTENING = 'rgba(0, 255, 100, '; // Green

        // --- ЛОГИКА CANVAS ---
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        let width, height;
        let particles = [];
        let currentState = 'idle'; // idle, listening, speaking

        function resize() {
            width = canvas.width = window.innerWidth;
            height = canvas.height = window.innerHeight;
        }
        window.addEventListener('resize', resize);
        resize();

        class Particle {
            constructor() {
                this.angle = Math.random() * Math.PI * 2;
                this.radius = Math.random() * CORE_RADIUS;
                this.size = Math.random() * 2 + 1;
                this.speed = Math.random() * 0.02 + 0.01;
                this.wobble = Math.random() * 10;
                this.x = 0;
                this.y = 0;
            }

            update(time) {
                // Вращение частиц по орбите
                let speedMultiplier = 1;
                let radiusMultiplier = 1;

                if (currentState === 'listening') {
                    speedMultiplier = 0.5;
                    radiusMultiplier = 1.5; // Расширяется
                } else if (currentState === 'speaking') {
                    speedMultiplier = 3; // Быстро крутится
                    radiusMultiplier = 0.8; // Сжимается
                }

                this.angle += this.speed * speedMultiplier;
                
                // Эффект дыхания
                const orbit = this.radius * radiusMultiplier + Math.sin(time * 0.002 + this.wobble) * 10;

                this.x = width / 2 + Math.cos(this.angle) * orbit;
                this.y = height / 2 + Math.sin(this.angle) * orbit;
            }

            draw() {
                let colorBase = COLOR_IDLE;
                if (currentState === 'speaking') colorBase = COLOR_SPEAKING;
                if (currentState === 'listening') colorBase = COLOR_LISTENING;

                ctx.fillStyle = colorBase + '0.8)';
                ctx.beginPath();
                ctx.arc(this.x, this.y, this.size, 0, Math.PI * 2);
                ctx.fill();
            }
        }

        function initParticles() {
            particles = [];
            for (let i = 0; i < PARTICLE_COUNT; i++) {
                particles.push(new Particle());
            }
        }
        initParticles();

        function animate(time) {
            ctx.clearRect(0, 0, width, height);

            // Соединительные линии (нейросеть)
            let colorBase = COLOR_IDLE;
            if (currentState === 'speaking') colorBase = COLOR_SPEAKING;
            if (currentState === 'listening') colorBase = COLOR_LISTENING;

            ctx.lineWidth = 0.5;
            
            for (let i = 0; i < particles.length; i++) {
                particles[i].update(time);
                particles[i].draw();

                // Рисуем линии между близкими частицами
                for (let j = i; j < particles.length; j++) {
                    const dx = particles[i].x - particles[j].x;
                    const dy = particles[i].y - particles[j].y;
                    const dist = Math.sqrt(dx * dx + dy * dy);

                    if (dist < CONNECT_DISTANCE) {
                        ctx.strokeStyle = colorBase + (1 - dist / CONNECT_DISTANCE) * 0.4 + ')';
                        ctx.beginPath();
                        ctx.moveTo(particles[i].x, particles[i].y);
                        ctx.lineTo(particles[j].x, particles[j].y);
                        ctx.stroke();
                    }
                }
            }
            requestAnimationFrame(animate);
        }
        requestAnimationFrame(animate);


        // --- ЛОГИКА АУДИО И СЕТИ ---
        const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
        const ws = new WebSocket(`${protocol}//${window.location.host}`);
        const status = document.getElementById('status');
        const clickZone = document.getElementById('click-zone');
        let audioContext;

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.lang = 'ru-RU';
        recognition.interimResults = false;

        clickZone.addEventListener('click', async () => {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                await audioContext.resume();
                status.innerHTML = "SYSTEM READY<br>LISTENING...";
            }

            try {
                recognition.start();
                currentState = 'listening';
                status.innerHTML = "LISTENING...";
            } catch (e) {
                console.log("Already listening");
            }
        });

        recognition.onresult = (event) => {
            const text = event.results[0][0].transcript;
            status.innerHTML = `USER INPUT:<br>"${text}"`;
            currentState = 'idle';
            ws.send(text);
        };

        recognition.onend = () => {
            if (currentState === 'listening') {
                currentState = 'idle';
                status.innerHTML = "PROCESSING...";
            }
        };

        ws.onmessage = async (event) => {
            if (typeof event.data === 'string') {
                const data = JSON.parse(event.data);
                if (data.type === 'text') {
                    // Пока ждем голос, выводим текст
                }
            } else {
                // Пришло аудио
                currentState = 'speaking';
                status.innerHTML = "AI SPEAKING...";
                
                const arrayBuffer = await event.data.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                source.start(0);

                source.onended = () => {
                    currentState = 'idle';
                    status.innerHTML = "AWAITING INPUT";
                };
            }
        };
    </script>
</body>
</html>